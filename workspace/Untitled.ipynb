{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aba38431-3dab-4761-bfd5-1a6def1525b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a354bf8-1900-41b0-8c71-96901e9ed070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19 µs, sys: 16 µs, total: 35 µs\n",
      "Wall time: 38.6 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def SparkIO(conf: SparkConf = SparkConf()):\n",
    "    app_name = conf.get(\"spark.app.name\")\n",
    "    master = conf.get(\"spark.master\")\n",
    "    print(f'Create SparkSession app {app_name} with {master} mode')\n",
    "    spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "    try:\n",
    "        yield spark\n",
    "    finally:\n",
    "        print(f'Stop SparkSession app {app_name}')\n",
    "        spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0957e5e9-254f-4ea6-9fe9-c566ffdf6ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_names = [\"albums_data\", \"artists_data\", \"tracks_data\", \"tracks_features_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34b265c-e538-4795-83e8-8d42ded14e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f32e5b97-59c4-421f-b2d1-b75ffb92b78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create SparkSession app ELT-app-2023-12-04 10:34:45.655302 with local[*] mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.4.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/04 10:34:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/12/04 10:34:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- external_urls: struct (nullable = true)\n",
      " |    |-- spotify: string (nullable = true)\n",
      " |-- followers: struct (nullable = true)\n",
      " |    |-- total: long (nullable = true)\n",
      " |-- genres: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- href: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- images: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- height: long (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- width: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- popularity: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- uri: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- album_group: string (nullable = true)\n",
      " |-- album_type: string (nullable = true)\n",
      " |-- artists: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- external_urls: struct (nullable = true)\n",
      " |    |    |    |-- spotify: string (nullable = true)\n",
      " |    |    |-- href: string (nullable = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |    |-- uri: string (nullable = true)\n",
      " |-- available_markets: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- external_urls: struct (nullable = true)\n",
      " |    |-- spotify: string (nullable = true)\n",
      " |-- href: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- images: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- height: long (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- width: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- release_date: string (nullable = true)\n",
      " |-- release_date_precision: string (nullable = true)\n",
      " |-- total_tracks: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- uri: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- artists: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- external_urls: struct (nullable = true)\n",
      " |    |    |    |-- spotify: string (nullable = true)\n",
      " |    |    |-- href: string (nullable = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |    |-- uri: string (nullable = true)\n",
      " |-- available_markets: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- disc_number: long (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      " |-- explicit: boolean (nullable = true)\n",
      " |-- external_urls: struct (nullable = true)\n",
      " |    |-- spotify: string (nullable = true)\n",
      " |-- href: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- is_local: boolean (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- preview_url: string (nullable = true)\n",
      " |-- track_number: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- uri: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- acousticness: double (nullable = true)\n",
      " |-- analysis_url: string (nullable = true)\n",
      " |-- danceability: double (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      " |-- energy: double (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- instrumentalness: double (nullable = true)\n",
      " |-- key: long (nullable = true)\n",
      " |-- liveness: double (nullable = true)\n",
      " |-- loudness: double (nullable = true)\n",
      " |-- mode: long (nullable = true)\n",
      " |-- speechiness: double (nullable = true)\n",
      " |-- tempo: double (nullable = true)\n",
      " |-- time_signature: long (nullable = true)\n",
      " |-- track_href: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- uri: string (nullable = true)\n",
      " |-- valence: double (nullable = true)\n",
      "\n",
      "None\n",
      "Stop SparkSession app ELT-app-2023-12-04 10:34:45.655302\n",
      "CPU times: user 65 ms, sys: 29.8 ms, total: 94.8 ms\n",
      "Wall time: 14.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "user = os.getenv(\"MONGODB_USER\")\n",
    "password = os.getenv(\"MONGODB_PASSWORD\")\n",
    "db = \"remake_spotify_crawling_data\"\n",
    "uri = f\"mongodb+srv://{user}:{password}@python.zynpktu.mongodb.net/?retryWrites=true&w=majority\"\n",
    "conf = (SparkConf().setAppName(\"ELT-app-{}\".format(datetime.today()))\n",
    "        .set(\"spark.executor.memory\", \"2g\")\n",
    "        .setMaster(\"local[*]\")\n",
    "        )\n",
    "\n",
    "with SparkIO(conf) as spark:\n",
    "    # album = spark.read.format(\"mongo\").option(\"uri\",uri).option('database', db).option('collection', table_names[0]).load()\n",
    "    # artist = spark.read.format(\"mongo\").option(\"uri\",uri).option('database', db).option('collection', table_names[1]).load()\n",
    "    # track = spark.read.format(\"mongo\").option(\"uri\",uri).option('database', db).option('collection', table_names[2]).load()\n",
    "    # track_feat = spark.read.format(\"mongo\").option(\"uri\",uri).option('database', db).option('collection', table_names[3]).load()\n",
    "    artist = spark.read.json(\"./data/artists_data.json\")\n",
    "    print(artist.printSchema())\n",
    "    artist = spark.read.json(\"./data/albums_data.json\")\n",
    "    print(artist.printSchema())\n",
    "    artist = spark.read.json(\"./data/tracks_data.json\")\n",
    "    print(artist.printSchema())\n",
    "    artist = spark.read.json(\"./data/tracks_features_data.json\")\n",
    "    print(artist.printSchema())\n",
    "    # print(f'len album:{album.count()}')\n",
    "    # print(f'len artist:{artist.count()}')\n",
    "    # print(f'len track:{track.count()}')\n",
    "    # print(f'len track feat:{track_feat.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebc54270-66b2-4692-a1eb-4baa7b33b6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/04 10:35:21 WARN FileSystem: Failed to initialize fileystem hdfs://namenode:8020/bronze_layer/artists_data.parquet: java.lang.IllegalArgumentException: java.net.UnknownHostException: namenode\n",
      "23/12/04 10:35:21 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: hdfs://namenode:8020/bronze_layer/artists_data.parquet.\n",
      "java.lang.IllegalArgumentException: java.net.UnknownHostException: namenode\n",
      "\tat org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:466)\n",
      "\tat org.apache.hadoop.hdfs.NameNodeProxiesClient.createProxyWithClientProtocol(NameNodeProxiesClient.java:134)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:374)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:308)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.initDFSClient(DistributedFileSystem.java:201)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:186)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:596)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.net.UnknownHostException: namenode\n",
      "\t... 31 more\n",
      "23/12/04 10:35:21 WARN FileSystem: Failed to initialize fileystem hdfs://namenode:8020/bronze_layer/artists_data.parquet: java.lang.IllegalArgumentException: java.net.UnknownHostException: namenode\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "java.net.UnknownHostException: namenode",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m table_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124martists_data\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m hdfs_uri \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhdfs://namenode:8020/bronze_layer/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m artist \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhdfs_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minferSchema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(artist\u001b[38;5;241m.\u001b[39mprintSchema())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/readwriter.py:301\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    295\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema, pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[1;32m    297\u001b[0m                recursiveFileLookup\u001b[38;5;241m=\u001b[39mrecursiveFileLookup, modifiedBefore\u001b[38;5;241m=\u001b[39mmodifiedBefore,\n\u001b[1;32m    298\u001b[0m                modifiedAfter\u001b[38;5;241m=\u001b[39mmodifiedAfter, datetimeRebaseMode\u001b[38;5;241m=\u001b[39mdatetimeRebaseMode,\n\u001b[1;32m    299\u001b[0m                int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode)\n\u001b[0;32m--> 301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: java.net.UnknownHostException: namenode"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "table_name='artists_data'\n",
    "hdfs_uri = f\"hdfs://namenode:8020/bronze_layer/{table_name}.parquet\"\n",
    "\n",
    "artist = spark.read.parquet(hdfs_uri, inferSchema=True)\n",
    "print(artist.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee6abef3-71bf-4ed8-b9f4-f5cd82f0c7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_id\n",
      "external_urls\n",
      "followers\n",
      "genres\n",
      "href\n",
      "id\n",
      "images\n",
      "name\n",
      "popularity\n",
      "type\n",
      "uri\n"
     ]
    }
   ],
   "source": [
    "for col in artist.schema.fields:\n",
    "    print(col.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a3ef23f-ba1b-4a59-bb72-237af5016c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
