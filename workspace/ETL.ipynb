{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37a95990-b807-498e-9b53-7eabfe755261",
   "metadata": {},
   "source": [
    "# PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d11de01-2d68-4e3f-a34b-e757fd0a22ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb78b08-f2d5-4fc8-abfa-0339adf3adcf",
   "metadata": {},
   "source": [
    "# IO manger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547663a2-cd79-4d1d-9a40-3c722c66e9e6",
   "metadata": {},
   "source": [
    "Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d974d2e-1f57-4624-8a64-8241b91ea991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def SparkIO(conf: SparkConf = SparkConf()):\n",
    "    app_name = conf.get(\"spark.app.name\")\n",
    "    master = conf.get(\"spark.master\")\n",
    "    print(f'Create SparkSession app {app_name} with {master} mode')\n",
    "    spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "    try:\n",
    "        yield spark\n",
    "    finally:\n",
    "        print(f'Stop SparkSession app {app_name}')\n",
    "        spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6828f914-a967-4c98-bc39-cf2a86639d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo.errors import ConnectionFailure\n",
    "from contextlib import contextmanager\n",
    "import os\n",
    "\n",
    "@contextmanager\n",
    "def MongodbIO():\n",
    "    user = os.getenv(\"MONGODB_USER\")\n",
    "    password = os.getenv(\"MONGODB_PASSWORD\")\n",
    "    uri = f\"mongodb+srv://{user}:{password}@python.zynpktu.mongodb.net/?retryWrites=true&w=majority\"\n",
    "    try:\n",
    "        client = MongoClient(uri)\n",
    "        print(f\"MongoDB Connected\")\n",
    "        yield client\n",
    "    except ConnectionFailure:\n",
    "        print(f\"Failed to connect with MongoDB\")\n",
    "        raise ConnectionFailure\n",
    "    finally:\n",
    "        print(\"Close connection to MongoDB\")\n",
    "        client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb9948d-4ad7-46ca-8716-560c613e3e24",
   "metadata": {},
   "source": [
    "## Bronze "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75d50f0a-a2a4-4aad-8294-d71a9c7b6b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSchema(df: pd.DataFrame):\n",
    "    \"\"\"This function create Pyspark Schema\"\"\"\n",
    "    field = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        dtype = str(df[col].dtype)\n",
    "\n",
    "        if dtype == 'object':\n",
    "            field_type = StringType()\n",
    "        elif 'int' in dtype:\n",
    "            field_type = IntegerType()\n",
    "        elif 'bool' in dtype: \n",
    "            field_type = BooleanType()\n",
    "        elif 'float' in dtype: \n",
    "            field_type = FloatType()\n",
    "        elif dtype == 'double':\n",
    "            field_type = DoubleType()\n",
    "        else:\n",
    "            field_type = StringType()\n",
    "        \n",
    "        field.append(StructField(col, field_type, True))\n",
    "\n",
    "    return StructType(field)\n",
    "\n",
    "\n",
    "def bronze_layer_task(spark: SparkSession, db: str, table_name: str) -> None:\n",
    "    \"\"\"Extract data from MongoDB to HDFS at bronze layer\"\"\"\n",
    "    user = os.getenv(\"MONGODB_USER\")\n",
    "    password = os.getenv(\"MONGODB_PASSWORD\")\n",
    "    hdfs_uri = f\"hdfs://namenode:8020/bronze_layer/{table_name}.parquet\"\n",
    "    uri = f\"mongodb+srv://{user}:{password}@python.zynpktu.mongodb.net/?retryWrites=true&w=majority\"\n",
    "    \n",
    "    # mongo_data = pd.DataFrame(list(collection.find({}, {\"_id\": 0}))) # eliminate the _id field\n",
    "    spark_data = spark.read.format(\"mongodb\").option(\"uri\", uri).option('database', db).option('collection', table_name).load()\n",
    "    \n",
    "    # try:\n",
    "    #     spark_data = spark.createDataFrame(mongo_data, schema=mongo_data.columns.tolist())\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error to Create Spark DataFrame {table_name} {e}\")\n",
    "    #     print(f\"Start Create Schema for {table_name}\")\n",
    "\n",
    "    #     schema = createSchema(mongo_data)\n",
    "    #     spark_data = spark.createDataFrame(mongo_data, schema=schema)\n",
    "    \n",
    "\n",
    "    print(f\"Writing {table_name}\")\n",
    "    try:\n",
    "        spark_data.write.parquet(hdfs_uri, mode=\"overwrite\")\n",
    "        print(f\"Bronze: Successfully push {table_name}.parquet\")\n",
    "    except Exception as e:\n",
    "        print(spark_data.printSchema())\n",
    "        print(e)\n",
    "\n",
    "def IngestHadoop(spark: SparkSession):\n",
    "    \"\"\"Extract data From MongoDb and Load to HDFS\"\"\"\n",
    "\n",
    "    # Connect to MongoDB\n",
    "\n",
    "    database_name = \"remake_spotify_crawling_data\"\n",
    "    # database_name = os.getenv(\"MONGODB_DATABASE\")\n",
    "    \n",
    "    \n",
    "    with MongodbIO() as client:\n",
    "        mongo_db = client[database_name] \n",
    "        collections = mongo_db.list_collection_names() #get all collectons\n",
    "    \n",
    "        #Running task concurrently\n",
    "        for collection in collections:\n",
    "            print(f\"{collection} start being Ingested...\")\n",
    "            bronze_layer_task(spark, database_name, collection) #collection is also the name of table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a68bbd-bcdc-436a-8233-c1631e7dcc5d",
   "metadata": {},
   "source": [
    "There is 4 tables:\n",
    "- artists_data.parquet\n",
    "- songs_data.parquet\n",
    "- genres_data.parquet\n",
    "- albums_data.parquet\n",
    "\n",
    "location: hdfs://namenode:8020/bronze_layer/{table_name}.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae751864-38fa-439c-86ba-25f8c5d0c5ef",
   "metadata": {},
   "source": [
    "## Silver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf39cf0-e570-41a0-be62-98be23c834a7",
   "metadata": {},
   "source": [
    "### Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e29de1-4be3-4ea6-a908-ab545adb3c48",
   "metadata": {},
   "source": [
    "![Schema](./spotify.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c92ca9-4f92-461c-819a-2f6c2c70278a",
   "metadata": {},
   "source": [
    "Target: \n",
    "- using pyspark Cleaning, droping duplicated, drop unusable column (Read [EDA](https://colab.research.google.com/drive/15uM8Uvj1I89zjtJrVn-Z7mvkfyCWo50T?usp=sharing)), format type, there are many duplicated observation.\n",
    "- join dim artist and dim albums -> join_artist_albums table (clean table before merge) (task 1)\n",
    "- clean genre, then write back to silver(task 2) -> clean_genre table\n",
    "- clean songs (task 3) -> clean_songs table (return None)\n",
    "- The location of silver: hdfs_uri = f\"hdfs://namenode:8020/silver_layer/{table_name}.parquet\" with table_name is name of result table\n",
    "\n",
    "\n",
    "Requirements:\n",
    "- Input of silver main task (spark session), Output: None\n",
    "- silver main task may have many child tasks, concurrently or sequencially\n",
    "- Child task input (spark session), any extended params or return base on you, ensure write back result in hdfs with related uri\n",
    "- Writing (print out) logs every action, handle error and exception (raise it if neccesary)\n",
    "\n",
    "Dont forget to add your main task to main function !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d88006a-e33c-403e-b407-1135f16817de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run some code here\n",
    "def silver_layer_task(spark: SparkSession):\n",
    "    '''Do some Cleaning tasks for silver layer'''\n",
    "    # task 1\n",
    "    # task 2\n",
    "    # task 3 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dabde27-5933-4ef3-8175-e04b07d83cfd",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fb1e0e4-6556-4a1c-a220-acd50f895285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_B():\n",
    "    \"\"\"ELT pipeline with pyspark\"\"\"\n",
    "\n",
    "    user = os.getenv(\"MONGODB_USER\")\n",
    "    password = os.getenv(\"MONGODB_PASSWORD\")\n",
    "    uri = f\"mongodb+srv://{user}:{password}@python.zynpktu.mongodb.net/?retryWrites=true&w=majority\"\n",
    "    conf = (SparkConf().setAppName(\"ETL-app-{}\".format(datetime.today()))\n",
    "        .set(\"spark.executor.memory\", \"8g\")\n",
    "        .set(\"spark.mongodb.read.connection.uri\",uri)\n",
    "        .set(\"spark.mongodb.write.connection.uri\", uri)\n",
    "        .set(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.2.1\")\n",
    "        .setMaster(\"local[*]\")\n",
    "        )\n",
    "\n",
    "    with SparkIO(conf) as spark:\n",
    "        IngestHadoop(spark) # <----- bronze task\n",
    "        # add silver tasks here <-------\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f14c330-c89d-4a78-99b7-dc627e2b1704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create SparkSession app ETL-app-2023-11-30 08:50:34.862617 with local[*] mode\n",
      "MongoDB Connected\n",
      "tracks_data start being Ingested...\n",
      "Writing tracks_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/30 08:50:39 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze: Successfully push tracks_data.parquet\n",
      "tracks_features_data start being Ingested...\n",
      "Writing tracks_features_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze: Successfully push tracks_features_data.parquet\n",
      "artists_data start being Ingested...\n",
      "Writing artists_data\n",
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- external_urls: struct (nullable = true)\n",
      " |    |-- spotify: string (nullable = true)\n",
      " |-- followers: struct (nullable = true)\n",
      " |    |-- href: void (nullable = true)\n",
      " |    |-- total: integer (nullable = true)\n",
      " |-- genres: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- href: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- images: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- height: integer (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- width: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- popularity: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- uri: string (nullable = true)\n",
      "\n",
      "None\n",
      "Parquet data source does not support struct<href:void,total:int> data type.\n",
      "albums_data start being Ingested...\n",
      "Writing albums_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze: Successfully push albums_data.parquet\n",
      "Close connection to MongoDB\n",
      "Stop SparkSession app ETL-app-2023-11-30 08:50:34.862617\n",
      "CPU times: user 321 ms, sys: 116 ms, total: 437 ms\n",
      "Wall time: 2min 38s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipeline_B()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
