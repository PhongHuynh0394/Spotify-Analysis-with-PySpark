{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37a95990-b807-498e-9b53-7eabfe755261",
   "metadata": {},
   "source": [
    "# PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d11de01-2d68-4e3f-a34b-e757fd0a22ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, BooleanType, StringType, IntegerType, LongType, ArrayType, DateType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb78b08-f2d5-4fc8-abfa-0339adf3adcf",
   "metadata": {},
   "source": [
    "# IO manger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547663a2-cd79-4d1d-9a40-3c722c66e9e6",
   "metadata": {},
   "source": [
    "Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d974d2e-1f57-4624-8a64-8241b91ea991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def SparkIO(conf: SparkConf = SparkConf()):\n",
    "    app_name = conf.get(\"spark.app.name\")\n",
    "    master = conf.get(\"spark.master\")\n",
    "    print(f'Create SparkSession app {app_name} with {master} mode')\n",
    "    spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "    try:\n",
    "        yield spark\n",
    "    finally:\n",
    "        print(f'Stop SparkSession app {app_name}')\n",
    "        spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6828f914-a967-4c98-bc39-cf2a86639d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo.errors import ConnectionFailure\n",
    "from contextlib import contextmanager\n",
    "import os\n",
    "\n",
    "@contextmanager\n",
    "def MongodbIO():\n",
    "    user = os.getenv(\"MONGODB_USER\")\n",
    "    password = os.getenv(\"MONGODB_PASSWORD\")\n",
    "    uri = f\"mongodb+srv://{user}:{password}@python.zynpktu.mongodb.net/?retryWrites=true&w=majority\"\n",
    "    try:\n",
    "        client = MongoClient(uri)\n",
    "        print(f\"MongoDB Connected\")\n",
    "        yield client\n",
    "    except ConnectionFailure:\n",
    "        print(f\"Failed to connect with MongoDB\")\n",
    "        raise ConnectionFailure\n",
    "    finally:\n",
    "        print(\"Close connection to MongoDB\")\n",
    "        client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb9948d-4ad7-46ca-8716-560c613e3e24",
   "metadata": {},
   "source": [
    "## Bronze "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "190ba553-98b8-4884-9899-4b95b7b9e9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSchema(table_name):\n",
    "    \"\"\"This function create Pyspark Schema\"\"\"\n",
    "    artist_schema = StructType([\n",
    "        StructField(\"_id\", StringType(), True),\n",
    "        StructField(\n",
    "            \"external_urls\",\n",
    "            StructType([\n",
    "                StructField(\"spotify\", StringType(), True)\n",
    "            ])\n",
    "        ),\n",
    "        StructField(\n",
    "            \"followers\", \n",
    "            StructType([\n",
    "                StructField(\"href\", StringType(), True),\n",
    "                StructField(\"total\", IntegerType(), True)\n",
    "            ])\n",
    "        ),\n",
    "        StructField(\n",
    "            \"genres\",\n",
    "            ArrayType(StringType(), True)      \n",
    "        ),\n",
    "        StructField(\"href\", StringType(), True),\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\n",
    "            \"images\",\n",
    "            ArrayType(\n",
    "                StructType([\n",
    "                    StructField(\"height\", IntegerType(), True),\n",
    "                    StructField(\"url\", StringType(), True),\n",
    "                    StructField(\"width\", IntegerType(), True)\n",
    "                ])\n",
    "            )\n",
    "        ),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"popularity\", IntegerType(), True),\n",
    "        StructField(\"type\", StringType(), True),\n",
    "        StructField(\"uri\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    album_schema = StructType([\n",
    "        StructField(\"_id\", StringType(), True),\n",
    "        StructField(\"album_type\", StringType(), True),\n",
    "        StructField(\n",
    "            \"copyrights\",\n",
    "            ArrayType(\n",
    "                StructType([\n",
    "                    StructField(\"text\", StringType(), True),\n",
    "                    StructField(\"type\", StringType(), True)\n",
    "                ])\n",
    "            )\n",
    "        ),\n",
    "        StructField(\n",
    "            \"external_ids\",\n",
    "            StructType([\n",
    "                StructField(\"upc\", StringType(), True)\n",
    "            ])\n",
    "        ),\n",
    "        StructField(\n",
    "            \"external_urls\",\n",
    "            StructType([\n",
    "                StructField(\"spotify\", StringType(), True)\n",
    "            ])\n",
    "        ),\n",
    "        StructField(\n",
    "            \"genres\",\n",
    "            ArrayType(StringType(), True)\n",
    "        ),\n",
    "        StructField(\"href\", StringType(), True),\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\n",
    "            \"images\",\n",
    "            ArrayType(\n",
    "                StructType([\n",
    "                    StructField(\"height\", IntegerType(), True),\n",
    "                    StructField(\"url\", StringType(), True),\n",
    "                    StructField(\"width\", IntegerType(), True)\n",
    "                ])\n",
    "            )\n",
    "        ),\n",
    "        StructField(\"label\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"popularity\", IntegerType(), True),\n",
    "        StructField(\"release_date\", StringType(), True),\n",
    "        StructField(\"release_date_precision\", StringType(), True),\n",
    "        StructField(\"total_tracks\", IntegerType(), True),\n",
    "        StructField(\"type\", StringType(), True),\n",
    "        StructField(\"uri\", StringType(), True),\n",
    "        StructField(\"artist_id\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    track_schema = StructType([\n",
    "        StructField(\"_id\", StringType(), True),\n",
    "        StructField(\"disc_number\", IntegerType(), True),\n",
    "        StructField(\"duration_ms\", LongType(), True),\n",
    "        StructField(\"explicit\", BooleanType(), True),\n",
    "        StructField(\n",
    "            \"external_ids\",\n",
    "            StructType([\n",
    "                StructField(\"isrc\", StringType(), True)\n",
    "            ])\n",
    "        ),\n",
    "        StructField(\n",
    "            \"external_urls\",\n",
    "            StructType([\n",
    "                StructField(\"spotify\", StringType(), True)\n",
    "            ])\n",
    "        ),\n",
    "        StructField(\"href\", StringType(), True),\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"is_local\", BooleanType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"popularity\", IntegerType(), True),\n",
    "        StructField(\"preview_url\", StringType(), True),\n",
    "        StructField(\"track_number\", IntegerType(), True),\n",
    "        StructField(\"type\", StringType(), True),\n",
    "        StructField(\"uri\", StringType(), True),\n",
    "        StructField(\"artist_id\", StringType(), True),\n",
    "        StructField(\"album_id\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    track_features_schema = StructType([\n",
    "        StructField(\"_id\", StringType(), True),\n",
    "        StructField(\"danceability\", DoubleType(), True),\n",
    "        StructField(\"energy\", DoubleType(), True),\n",
    "        StructField(\"key\", IntegerType(), True),\n",
    "        StructField(\"loudness\", DoubleType(), True),\n",
    "        StructField(\"mode\", IntegerType(), True),\n",
    "        StructField(\"speechiness\", DoubleType(), True),\n",
    "        StructField(\"acousticness\", DoubleType(), True),\n",
    "        StructField(\"instrumentalness\", DoubleType(), True),\n",
    "        StructField(\"liveness\", DoubleType(), True),\n",
    "        StructField(\"valence\", DoubleType(), True),\n",
    "        StructField(\"tempo\", DoubleType(), True),\n",
    "        StructField(\"type\", StringType(), True),\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"uri\", StringType(), True),\n",
    "        StructField(\"track_href\", StringType(), True),\n",
    "        StructField(\"analysis_url\", StringType(), True),\n",
    "        StructField(\"duration_ms\", LongType(), True),\n",
    "        StructField(\"time_signature\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "    if 'artist' in table_name:\n",
    "        return artist_schema\n",
    "    elif 'album' in table_name:\n",
    "        return album_schema\n",
    "    elif 'feature' in table_name:\n",
    "        return track_features_schema\n",
    "    else:\n",
    "        return track_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75d50f0a-a2a4-4aad-8294-d71a9c7b6b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bronze_layer_task(spark: SparkSession, database_name: str, table_name: str) -> None:\n",
    "    \"\"\"Extract data from MongoDB to HDFS at bronze layer\"\"\"\n",
    "    user = os.getenv(\"MONGODB_USER\")\n",
    "    password = os.getenv(\"MONGODB_PASSWORD\")\n",
    "    hdfs_uri = f\"hdfs://namenode:8020/bronze_layer/{table_name}.parquet\"\n",
    "    mongo_uri = f\"mongodb+srv://{user}:{password}@python.zynpktu.mongodb.net/?retryWrites=true&w=majority\"\n",
    "    \n",
    "    spark_data = (spark.read.format(\"mongodb\")\n",
    "              .schema(getSchema(table_name))\n",
    "              .option(\"uri\", mongo_uri)\n",
    "              .option('database', database_name)\n",
    "              .option('collection', table_name)\n",
    "              .load()\n",
    "              .select([col for col in getSchema(table_name).fieldNames() if col != '_id'])\n",
    "              )\n",
    "    # Exclude _id field\n",
    "    print(f\"Writing {table_name}\")\n",
    "    try:\n",
    "        spark_data.write.parquet(hdfs_uri, mode=\"overwrite\")\n",
    "        print(f\"Bronze: Successfully push {table_name}.parquet\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(spark_data.printSchema())\n",
    "        # spark_data = (spark.read.format(\"mongodb\")\n",
    "        #       .schema(getSchema(table_name))\n",
    "        #       .option(\"uri\", mongo_uri)\n",
    "        #       .option('database', database_name)\n",
    "        #       .option('collection', table_name)\n",
    "        #       .load()\n",
    "        #       .select([col for col in getSchema(table_name).fieldNames() if col != '_id'])\n",
    "        #       )\n",
    "        # spark_data.write.parquet(hdfs_uri, mode=\"overwrite\")\n",
    "        # print(f\"Bronze: Successfully push {table_name}.parquet\")\n",
    "\n",
    "def IngestHadoop(spark: SparkSession):\n",
    "    \"\"\"Extract data From MongoDb and Load to HDFS\"\"\"\n",
    "\n",
    "    # Connect to MongoDB\n",
    "\n",
    "    # database_name = \"remake_spotify_crawling_data\"\n",
    "    database_name = os.getenv(\"MONGODB_DATABASE\")\n",
    "    \n",
    "    \n",
    "    with MongodbIO() as client:\n",
    "        mongo_db = client[database_name] \n",
    "        collections = mongo_db.list_collection_names() #get all collectons\n",
    "    \n",
    "        #Running task concurrently\n",
    "        for collection in collections:\n",
    "            print(f\"{collection} start being Ingested...\")\n",
    "            bronze_layer_task(spark, database_name, collection) #collection is also the name of table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a68bbd-bcdc-436a-8233-c1631e7dcc5d",
   "metadata": {},
   "source": [
    "There is 4 tables:\n",
    "- artists_data.parquet\n",
    "- songs_data.parquet\n",
    "- genres_data.parquet\n",
    "- albums_data.parquet\n",
    "\n",
    "location: hdfs://namenode:8020/bronze_layer/{table_name}.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae751864-38fa-439c-86ba-25f8c5d0c5ef",
   "metadata": {},
   "source": [
    "## Silver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf39cf0-e570-41a0-be62-98be23c834a7",
   "metadata": {},
   "source": [
    "### Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e29de1-4be3-4ea6-a908-ab545adb3c48",
   "metadata": {},
   "source": [
    "![Schema](./spotify.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c92ca9-4f92-461c-819a-2f6c2c70278a",
   "metadata": {},
   "source": [
    "Target: \n",
    "- using pyspark Cleaning, droping duplicated, drop unusable column (Read [EDA](https://colab.research.google.com/drive/15uM8Uvj1I89zjtJrVn-Z7mvkfyCWo50T?usp=sharing)), format type, there are many duplicated observation.\n",
    "- join dim artist and dim albums -> join_artist_albums table (clean table before merge) (task 1)\n",
    "- clean genre, then write back to silver(task 2) -> clean_genre table\n",
    "- clean songs (task 3) -> clean_songs table (return None)\n",
    "- The location of silver: hdfs_uri = f\"hdfs://namenode:8020/silver_layer/{table_name}.parquet\" with table_name is name of result table\n",
    "\n",
    "\n",
    "Requirements:\n",
    "- Input of silver main task (spark session), Output: None\n",
    "- silver main task may have many child tasks, concurrently or sequencially\n",
    "- Child task input (spark session), any extended params or return base on you, ensure write back result in hdfs with related uri\n",
    "- Writing (print out) logs every action, handle error and exception (raise it if neccesary)\n",
    "\n",
    "Dont forget to add your main task to main function !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d88006a-e33c-403e-b407-1135f16817de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run some code here\n",
    "def silver_layer_task(spark: SparkSession):\n",
    "    '''Do some Cleaning tasks for silver layer'''\n",
    "    # task 1\n",
    "    # task 2\n",
    "    # task 3 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dabde27-5933-4ef3-8175-e04b07d83cfd",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fb1e0e4-6556-4a1c-a220-acd50f895285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_B():\n",
    "    \"\"\"ELT pipeline with pyspark\"\"\"\n",
    "\n",
    "    user = os.getenv(\"MONGODB_USER\")\n",
    "    password = os.getenv(\"MONGODB_PASSWORD\")\n",
    "    uri = f\"mongodb+srv://{user}:{password}@python.zynpktu.mongodb.net/?retryWrites=true&w=majority\"\n",
    "    conf = (SparkConf().setAppName(\"ETL-app-{}\".format(datetime.today()))\n",
    "        .set(\"spark.executor.memory\", \"2g\")\n",
    "        .set(\"spark.mongodb.read.connection.uri\",uri)\n",
    "        .set(\"spark.mongodb.write.connection.uri\", uri)\n",
    "        .set(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.2.1\")\n",
    "        .setMaster(\"local[*]\")\n",
    "        )\n",
    "\n",
    "    with SparkIO(conf) as spark:\n",
    "        IngestHadoop(spark) # <----- bronze task\n",
    "        # add silver tasks here <-------\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f14c330-c89d-4a78-99b7-dc627e2b1704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create SparkSession app ETL-app-2023-12-04 10:38:41.571238 with local[*] mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.4.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-7b76f791-c9f1-4d6c-8833-707f81420baa;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;10.2.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.8.2 in central\n",
      "\t[4.8.2] org.mongodb#mongodb-driver-sync;[4.8.1,4.8.99)\n",
      "\tfound org.mongodb#bson;4.8.2 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.8.2 in central\n",
      "\tfound org.mongodb#bson-record-codec;4.8.2 in central\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.12/10.2.1/mongo-spark-connector_2.12-10.2.1.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb.spark#mongo-spark-connector_2.12;10.2.1!mongo-spark-connector_2.12.jar (712ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/4.8.2/mongodb-driver-sync-4.8.2.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#mongodb-driver-sync;4.8.2!mongodb-driver-sync.jar (397ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/bson/4.8.2/bson-4.8.2.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#bson;4.8.2!bson.jar (637ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-core/4.8.2/mongodb-driver-core-4.8.2.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#mongodb-driver-core;4.8.2!mongodb-driver-core.jar (560ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/bson-record-codec/4.8.2/bson-record-codec-4.8.2.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#bson-record-codec;4.8.2!bson-record-codec.jar (202ms)\n",
      ":: resolution report :: resolve 12502ms :: artifacts dl 2516ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.8.2 from central in [default]\n",
      "\torg.mongodb#bson-record-codec;4.8.2 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.8.2 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.8.2 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;10.2.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   5   |   5   |   0   ||   5   |   5   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-7b76f791-c9f1-4d6c-8833-707f81420baa\n",
      "\tconfs: [default]\n",
      "\t5 artifacts copied, 0 already retrieved (2370kB/13ms)\n",
      "23/12/04 10:38:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB Connected\n",
      "tracks_features_data start being Ingested...\n",
      "Writing tracks_features_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/04 10:39:06 WARN FileSystem: Failed to initialize fileystem hdfs://namenode:8020/bronze_layer/tracks_features_data.parquet: java.lang.IllegalArgumentException: java.net.UnknownHostException: namenode\n",
      "23/12/04 10:39:06 WARN FileSystem: Failed to initialize fileystem hdfs://namenode:8020/bronze_layer/artists_data.parquet: java.lang.IllegalArgumentException: java.net.UnknownHostException: namenode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java.net.UnknownHostException: namenode\n",
      "root\n",
      " |-- danceability: double (nullable = true)\n",
      " |-- energy: double (nullable = true)\n",
      " |-- key: integer (nullable = true)\n",
      " |-- loudness: double (nullable = true)\n",
      " |-- mode: integer (nullable = true)\n",
      " |-- speechiness: double (nullable = true)\n",
      " |-- acousticness: double (nullable = true)\n",
      " |-- instrumentalness: double (nullable = true)\n",
      " |-- liveness: double (nullable = true)\n",
      " |-- valence: double (nullable = true)\n",
      " |-- tempo: double (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- uri: string (nullable = true)\n",
      " |-- track_href: string (nullable = true)\n",
      " |-- analysis_url: string (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      " |-- time_signature: integer (nullable = true)\n",
      "\n",
      "None\n",
      "artists_data start being Ingested...\n",
      "Writing artists_data\n",
      "java.net.UnknownHostException: namenode\n",
      "root\n",
      " |-- external_urls: struct (nullable = true)\n",
      " |    |-- spotify: string (nullable = true)\n",
      " |-- followers: struct (nullable = true)\n",
      " |    |-- href: string (nullable = true)\n",
      " |    |-- total: integer (nullable = true)\n",
      " |-- genres: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- href: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- images: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- height: integer (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- width: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- popularity: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- uri: string (nullable = true)\n",
      "\n",
      "None\n",
      "tracks_data start being Ingested...\n",
      "Writing tracks_data\n",
      "java.net.UnknownHostException: namenode\n",
      "root\n",
      " |-- disc_number: integer (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      " |-- explicit: boolean (nullable = true)\n",
      " |-- external_ids: struct (nullable = true)\n",
      " |    |-- isrc: string (nullable = true)\n",
      " |-- external_urls: struct (nullable = true)\n",
      " |    |-- spotify: string (nullable = true)\n",
      " |-- href: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- is_local: boolean (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- popularity: integer (nullable = true)\n",
      " |-- preview_url: string (nullable = true)\n",
      " |-- track_number: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- uri: string (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- album_id: string (nullable = true)\n",
      "\n",
      "None\n",
      "albums_data start being Ingested...\n",
      "Writing albums_data\n",
      "java.net.UnknownHostException: namenode\n",
      "root\n",
      " |-- album_type: string (nullable = true)\n",
      " |-- copyrights: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- text: string (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |-- external_ids: struct (nullable = true)\n",
      " |    |-- upc: string (nullable = true)\n",
      " |-- external_urls: struct (nullable = true)\n",
      " |    |-- spotify: string (nullable = true)\n",
      " |-- genres: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- href: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- images: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- height: integer (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- width: integer (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- popularity: integer (nullable = true)\n",
      " |-- release_date: string (nullable = true)\n",
      " |-- release_date_precision: string (nullable = true)\n",
      " |-- total_tracks: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- uri: string (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      "\n",
      "None\n",
      "Close connection to MongoDB\n",
      "Stop SparkSession app ETL-app-2023-12-04 10:38:41.571238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/04 10:39:06 WARN FileSystem: Failed to initialize fileystem hdfs://namenode:8020/bronze_layer/tracks_data.parquet: java.lang.IllegalArgumentException: java.net.UnknownHostException: namenode\n",
      "23/12/04 10:39:06 WARN FileSystem: Failed to initialize fileystem hdfs://namenode:8020/bronze_layer/albums_data.parquet: java.lang.IllegalArgumentException: java.net.UnknownHostException: namenode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 104 ms, sys: 112 ms, total: 216 ms\n",
      "Wall time: 25.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/04 10:40:11 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/spark-9bce2320-fc6c-406c-b34b-4f970cfef456/pyspark-3df822b5-5afd-418c-808d-c6a73d2967ad. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/spark-9bce2320-fc6c-406c-b34b-4f970cfef456/pyspark-3df822b5-5afd-418c-808d-c6a73d2967ad\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:171)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:110)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1193)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2048)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipeline_B()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6b2c652-689b-43a2-b910-4ca0ad8215cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.4.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/08 12:39:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "172478"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = (SparkConf().setAppName(\"ETL-app-{}\".format(datetime.today()))\n",
    "        .set(\"spark.executor.memory\", \"2g\")\n",
    "        .setMaster(\"local[*]\")\n",
    "        )\n",
    "\n",
    "table_name = 'gold_tracks'\n",
    "layer = 'gold_layer'\n",
    "hdfs_uri = f\"hdfs://namenode:8020/{layer}/{table_name}.parquet\"\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "df = spark.read.parquet(hdfs_uri, inferSchema=True)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d75a5b12-46fd-4e4f-9647-064bc32969d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1518"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e94fdc8-953a-4eee-a814-32d4df84195a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading streamlit-1.29.0-py2.py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Downloading altair-5.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from streamlit) (1.6.3)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit)\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.11/site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: importlib-metadata<7,>=1.4 in /opt/conda/lib/python3.11/site-packages (from streamlit) (6.8.0)\n",
      "Requirement already satisfied: numpy<2,>=1.19.3 in /opt/conda/lib/python3.11/site-packages (from streamlit) (1.26.2)\n",
      "Requirement already satisfied: packaging<24,>=16.8 in /opt/conda/lib/python3.11/site-packages (from streamlit) (23.2)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in /opt/conda/lib/python3.11/site-packages (from streamlit) (2.1.3)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in /opt/conda/lib/python3.11/site-packages (from streamlit) (10.1.0)\n",
      "Requirement already satisfied: protobuf<5,>=3.20 in /opt/conda/lib/python3.11/site-packages (from streamlit) (4.25.1)\n",
      "Requirement already satisfied: pyarrow>=6.0 in /opt/conda/lib/python3.11/site-packages (from streamlit) (13.0.0)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.3 in /opt/conda/lib/python3.11/site-packages (from streamlit) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /opt/conda/lib/python3.11/site-packages (from streamlit) (2.31.0)\n",
      "Collecting rich<14,>=10.14.0 (from streamlit)\n",
      "  Downloading rich-13.7.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting tenacity<9,>=8.1.0 (from streamlit)\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /opt/conda/lib/python3.11/site-packages (from streamlit) (4.8.0)\n",
      "Collecting tzlocal<6,>=1.1 (from streamlit)\n",
      "  Downloading tzlocal-5.2-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting validators<1,>=0.2 (from streamlit)\n",
      "  Downloading validators-0.22.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/conda/lib/python3.11/site-packages (from streamlit) (3.1.40)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m455.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /opt/conda/lib/python3.11/site-packages (from streamlit) (6.3.3)\n",
      "Collecting watchdog>=2.1.5 (from streamlit)\n",
      "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (4.19.1)\n",
      "Collecting toolz (from altair<6,>=4.0->streamlit)\n",
      "  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.11/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata<7,>=1.4->streamlit) (3.17.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas<3,>=1.3.0->streamlit) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas<3,>=1.3.0->streamlit) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (2023.7.22)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14,>=10.14.0->streamlit)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.10.6)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading streamlit-1.29.0-py2.py3-none-any.whl (8.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m878.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading altair-5.2.0-py3-none-any.whl (996 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m996.9/996.9 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Downloading rich-13.7.0-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.6/240.6 kB\u001b[0m \u001b[31m955.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Downloading tzlocal-5.2-py3-none-any.whl (17 kB)\n",
      "Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m559.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: watchdog, validators, tzlocal, toolz, toml, tenacity, mdurl, cachetools, pydeck, markdown-it-py, rich, altair, streamlit\n",
      "Successfully installed altair-5.2.0 cachetools-5.3.2 markdown-it-py-3.0.0 mdurl-0.1.2 pydeck-0.8.1b0 rich-13.7.0 streamlit-1.29.0 tenacity-8.2.3 toml-0.10.2 toolz-0.12.0 tzlocal-5.2 validators-0.22.0 watchdog-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f3b3cf9-df90-4e6b-b143-ae482a581f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2b0e730-b7c7-4d59-b6ae-4d9ac421b24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- danceability: double (nullable = true)\n",
      " |-- energy: double (nullable = true)\n",
      " |-- key: integer (nullable = true)\n",
      " |-- loudness: double (nullable = true)\n",
      " |-- mode: integer (nullable = true)\n",
      " |-- speechiness: double (nullable = true)\n",
      " |-- acousticness: double (nullable = true)\n",
      " |-- instrumentalness: double (nullable = true)\n",
      " |-- liveness: double (nullable = true)\n",
      " |-- valence: double (nullable = true)\n",
      " |-- tempo: double (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- uri: string (nullable = true)\n",
      " |-- track_href: string (nullable = true)\n",
      " |-- analysis_url: string (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      " |-- time_signature: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
