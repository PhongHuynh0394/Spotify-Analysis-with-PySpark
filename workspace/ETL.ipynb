{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37a95990-b807-498e-9b53-7eabfe755261",
   "metadata": {},
   "source": [
    "# PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d11de01-2d68-4e3f-a34b-e757fd0a22ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, BooleanType, StringType, IntegerType, LongType, ArrayType, DateType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb78b08-f2d5-4fc8-abfa-0339adf3adcf",
   "metadata": {},
   "source": [
    "# IO manger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547663a2-cd79-4d1d-9a40-3c722c66e9e6",
   "metadata": {},
   "source": [
    "Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d974d2e-1f57-4624-8a64-8241b91ea991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def SparkIO(conf: SparkConf = SparkConf()):\n",
    "    app_name = conf.get(\"spark.app.name\")\n",
    "    master = conf.get(\"spark.master\")\n",
    "    print(f'Create SparkSession app {app_name} with {master} mode')\n",
    "    spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "    try:\n",
    "        yield spark\n",
    "    finally:\n",
    "        print(f'Stop SparkSession app {app_name}')\n",
    "        spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6828f914-a967-4c98-bc39-cf2a86639d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo.errors import ConnectionFailure\n",
    "from contextlib import contextmanager\n",
    "import os\n",
    "\n",
    "@contextmanager\n",
    "def MongodbIO():\n",
    "    user = os.getenv(\"MONGODB_USER\")\n",
    "    password = os.getenv(\"MONGODB_PASSWORD\")\n",
    "    uri = f\"mongodb+srv://{user}:{password}@python.zynpktu.mongodb.net/?retryWrites=true&w=majority\"\n",
    "    try:\n",
    "        client = MongoClient(uri)\n",
    "        print(f\"MongoDB Connected\")\n",
    "        yield client\n",
    "    except ConnectionFailure:\n",
    "        print(f\"Failed to connect with MongoDB\")\n",
    "        raise ConnectionFailure\n",
    "    finally:\n",
    "        print(\"Close connection to MongoDB\")\n",
    "        client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb9948d-4ad7-46ca-8716-560c613e3e24",
   "metadata": {},
   "source": [
    "## Bronze "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "190ba553-98b8-4884-9899-4b95b7b9e9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSchema(table_name):\n",
    "    \"\"\"This function create Pyspark Schema\"\"\"\n",
    "    artist_schema = StructType([\n",
    "    StructField(\n",
    "        \"_id\", StringType(), True\n",
    "    ),\n",
    "    StructField(\n",
    "        \"external_urls\",\n",
    "        StructType([\n",
    "            StructField(\"spotify\", StringType(), True)\n",
    "        ])\n",
    "    ),\n",
    "    StructField(\n",
    "        \"followers\", \n",
    "        StructType([\n",
    "            StructField(\"href\", StringType(), True),\n",
    "            StructField(\"total\", IntegerType(), True)\n",
    "        ])\n",
    "    ),\n",
    "    StructField(\n",
    "        \"genres\",\n",
    "        ArrayType(StringType(), True)      \n",
    "    ),\n",
    "    StructField(\"href\", StringType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\n",
    "        \"images\",\n",
    "        ArrayType(\n",
    "            StructType([\n",
    "                StructField(\"height\", IntegerType(), True),\n",
    "                StructField(\"url\", StringType(), True),\n",
    "                StructField(\"width\", IntegerType(), True)\n",
    "            ])\n",
    "        )\n",
    "    ),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"popularity\", IntegerType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"uri\", StringType(), True)\n",
    "])\n",
    "\n",
    "    album_schema = StructType([\n",
    "   StructField(\n",
    "        \"_id\", StringType(), True\n",
    "    ),\n",
    "    StructField(\"album_group\", StringType(), True),\n",
    "    StructField(\"album_type\", StringType(), True),\n",
    "    StructField(\n",
    "        \"artists\",\n",
    "        ArrayType(\n",
    "            StructType([\n",
    "                StructField(\n",
    "                    \"external_urls\",\n",
    "                    StructType([\n",
    "                        StructField(\"spotify\", StringType(), True)\n",
    "                    ])\n",
    "                ),\n",
    "                StructField(\"href\", StringType(), True),\n",
    "                StructField(\"id\", StringType(), True),\n",
    "                StructField(\"name\", StringType(), True),\n",
    "                StructField(\"type\", StringType(), True),\n",
    "                StructField(\"uri\", StringType(), True)\n",
    "            ])\n",
    "        )\n",
    "    ),\n",
    "    StructField(\n",
    "        \"available_markets\",\n",
    "        ArrayType(StringType(), True)\n",
    "    ),\n",
    "    StructField(\n",
    "        \"external_urls\",\n",
    "        StructType([\n",
    "            StructField(\"spotify\", StringType(), True)\n",
    "        ])\n",
    "    ),\n",
    "    StructField(\"href\", StringType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\n",
    "        \"images\",\n",
    "        ArrayType(\n",
    "            StructType([\n",
    "                StructField(\"height\", IntegerType(), True),\n",
    "                StructField(\"url\", StringType(), True),\n",
    "                StructField(\"width\", IntegerType(), True)\n",
    "            ])\n",
    "        )\n",
    "    ),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"release_date\", StringType(), True),\n",
    "    StructField(\"release_date_precision\", StringType(), True),\n",
    "    StructField(\"total_tracks\", IntegerType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"uri\", StringType(), True)\n",
    "])\n",
    "\n",
    "    track_schema = StructType([\n",
    "    StructField(\n",
    "        \"_id\", StringType(), True\n",
    "    ),\n",
    "    StructField(\n",
    "        \"artists\",\n",
    "        ArrayType(\n",
    "            StructType([\n",
    "                StructField(\n",
    "                    \"external_urls\",\n",
    "                    StructType([\n",
    "                        StructField(\"spotify\", StringType(), True)\n",
    "                    ])\n",
    "                ),\n",
    "                StructField(\"href\", StringType(), True),\n",
    "                StructField(\"id\", StringType(), True),\n",
    "                StructField(\"name\", StringType(), True),\n",
    "                StructField(\"type\", StringType(), True),\n",
    "                StructField(\"uri\", StringType(), True)\n",
    "            ])\n",
    "        )\n",
    "    ),\n",
    "    StructField(\n",
    "        \"available_markets\", \n",
    "        ArrayType(StringType(), True)\n",
    "    ),\n",
    "    StructField(\"disc_number\", IntegerType(), True),\n",
    "    StructField(\"duration_ms\", LongType(), True),\n",
    "    StructField(\"explicit\", BooleanType(), True),\n",
    "    StructField(\n",
    "        \"external_urls\",\n",
    "        StructType([\n",
    "            StructField(\"spotify\", StringType(), True)\n",
    "        ])\n",
    "    ),\n",
    "    StructField(\"href\", StringType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"is_local\", BooleanType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"preview_url\", StringType(), True),\n",
    "    StructField(\"track_number\", IntegerType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"uri\", StringType(), True)\n",
    "])\n",
    "\n",
    "    track_features_schema = StructType([\n",
    "    StructField(\n",
    "        \"_id\", StringType(), True\n",
    "    ),\n",
    "    StructField(\"danceability\", DoubleType(), True),\n",
    "    StructField(\"energy\", DoubleType(), True),\n",
    "    StructField(\"key\", IntegerType(), True),\n",
    "    StructField(\"loudness\", DoubleType(), True),\n",
    "    StructField(\"mode\", IntegerType(), True),\n",
    "    StructField(\"speechiness\", DoubleType(), True),\n",
    "    StructField(\"acousticness\", DoubleType(), True),\n",
    "    StructField(\"instrumentalness\", DoubleType(), True),\n",
    "    StructField(\"liveness\", DoubleType(), True),\n",
    "    StructField(\"valence\", DoubleType(), True),\n",
    "    StructField(\"tempo\", DoubleType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"uri\", StringType(), True),\n",
    "    StructField(\"track_href\", StringType(), True),\n",
    "    StructField(\"analysis_url\", StringType(), True),\n",
    "    StructField(\"duration_ms\", LongType(), True),\n",
    "    StructField(\"time_signature\", IntegerType(), True)\n",
    "])\n",
    "    if 'artist' in table_name:\n",
    "        return artist_schema\n",
    "    elif 'album' in table_name:\n",
    "        return album_schema\n",
    "    elif 'feature' in table_name:\n",
    "        return track_features_schema\n",
    "    else:\n",
    "        return track_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75d50f0a-a2a4-4aad-8294-d71a9c7b6b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bronze_layer_task(spark: SparkSession, db: str, table_name: str) -> None:\n",
    "    \"\"\"Extract data from MongoDB to HDFS at bronze layer\"\"\"\n",
    "    user = os.getenv(\"MONGODB_USER\")\n",
    "    password = os.getenv(\"MONGODB_PASSWORD\")\n",
    "    hdfs_uri = f\"hdfs://namenode:8020/bronze_layer/{table_name}.parquet\"\n",
    "    uri = f\"mongodb+srv://{user}:{password}@python.zynpktu.mongodb.net/?retryWrites=true&w=majority\"\n",
    "    \n",
    "    spark_data = (spark.read.format(\"mongodb\")\n",
    "              .schema(getSchema(table_name))\n",
    "              .option(\"uri\", mongo_uri)\n",
    "              .option('database', database_name)\n",
    "              .option('collection', table_name)\n",
    "              .load()\n",
    "              .select([col for col in getSchema(table_name).fieldNames() if col != '_id']) # exclude _id field\n",
    "              )\n",
    "\n",
    "    print(f\"Writing {table_name}\")\n",
    "    try:\n",
    "        spark_data.write.parquet(hdfs_uri, mode=\"overwrite\")\n",
    "        print(f\"Bronze: Successfully push {table_name}.parquet\")\n",
    "    except Exception as e:\n",
    "        print(spark_data.printSchema())\n",
    "        print(e)\n",
    "\n",
    "def IngestHadoop(spark: SparkSession):\n",
    "    \"\"\"Extract data From MongoDb and Load to HDFS\"\"\"\n",
    "\n",
    "    # Connect to MongoDB\n",
    "\n",
    "    database_name = \"remake_spotify_crawling_data\"\n",
    "    # database_name = os.getenv(\"MONGODB_DATABASE\")\n",
    "    \n",
    "    \n",
    "    with MongodbIO() as client:\n",
    "        mongo_db = client[database_name] \n",
    "        collections = mongo_db.list_collection_names() #get all collectons\n",
    "    \n",
    "        #Running task concurrently\n",
    "        for collection in collections:\n",
    "            print(f\"{collection} start being Ingested...\")\n",
    "            bronze_layer_task(spark, database_name, collection) #collection is also the name of table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a68bbd-bcdc-436a-8233-c1631e7dcc5d",
   "metadata": {},
   "source": [
    "There is 4 tables:\n",
    "- artists_data.parquet\n",
    "- songs_data.parquet\n",
    "- genres_data.parquet\n",
    "- albums_data.parquet\n",
    "\n",
    "location: hdfs://namenode:8020/bronze_layer/{table_name}.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae751864-38fa-439c-86ba-25f8c5d0c5ef",
   "metadata": {},
   "source": [
    "## Silver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf39cf0-e570-41a0-be62-98be23c834a7",
   "metadata": {},
   "source": [
    "### Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e29de1-4be3-4ea6-a908-ab545adb3c48",
   "metadata": {},
   "source": [
    "![Schema](./spotify.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c92ca9-4f92-461c-819a-2f6c2c70278a",
   "metadata": {},
   "source": [
    "Target: \n",
    "- using pyspark Cleaning, droping duplicated, drop unusable column (Read [EDA](https://colab.research.google.com/drive/15uM8Uvj1I89zjtJrVn-Z7mvkfyCWo50T?usp=sharing)), format type, there are many duplicated observation.\n",
    "- join dim artist and dim albums -> join_artist_albums table (clean table before merge) (task 1)\n",
    "- clean genre, then write back to silver(task 2) -> clean_genre table\n",
    "- clean songs (task 3) -> clean_songs table (return None)\n",
    "- The location of silver: hdfs_uri = f\"hdfs://namenode:8020/silver_layer/{table_name}.parquet\" with table_name is name of result table\n",
    "\n",
    "\n",
    "Requirements:\n",
    "- Input of silver main task (spark session), Output: None\n",
    "- silver main task may have many child tasks, concurrently or sequencially\n",
    "- Child task input (spark session), any extended params or return base on you, ensure write back result in hdfs with related uri\n",
    "- Writing (print out) logs every action, handle error and exception (raise it if neccesary)\n",
    "\n",
    "Dont forget to add your main task to main function !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d88006a-e33c-403e-b407-1135f16817de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run some code here\n",
    "def silver_layer_task(spark: SparkSession):\n",
    "    '''Do some Cleaning tasks for silver layer'''\n",
    "    # task 1\n",
    "    # task 2\n",
    "    # task 3 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dabde27-5933-4ef3-8175-e04b07d83cfd",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fb1e0e4-6556-4a1c-a220-acd50f895285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_B():\n",
    "    \"\"\"ELT pipeline with pyspark\"\"\"\n",
    "\n",
    "    user = os.getenv(\"MONGODB_USER\")\n",
    "    password = os.getenv(\"MONGODB_PASSWORD\")\n",
    "    uri = f\"mongodb+srv://{user}:{password}@python.zynpktu.mongodb.net/?retryWrites=true&w=majority\"\n",
    "    conf = (SparkConf().setAppName(\"ETL-app-{}\".format(datetime.today()))\n",
    "        .set(\"spark.executor.memory\", \"2g\")\n",
    "        .set(\"spark.mongodb.read.connection.uri\",uri)\n",
    "        .set(\"spark.mongodb.write.connection.uri\", uri)\n",
    "        .set(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.2.1\")\n",
    "        .setMaster(\"local[*]\")\n",
    "        )\n",
    "\n",
    "    with SparkIO(conf) as spark:\n",
    "        IngestHadoop(spark) # <----- bronze task\n",
    "        # add silver tasks here <-------\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f14c330-c89d-4a78-99b7-dc627e2b1704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create SparkSession app ETL-app-2023-12-02 09:40:29.208405 with local[*] mode\n",
      "MongoDB Connected\n",
      "tracks_data start being Ingested...\n",
      "Close connection to MongoDB\n",
      "Stop SparkSession app ETL-app-2023-12-02 09:40:29.208405\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mongo_uri' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m\n",
      "Cell \u001b[0;32mIn[21], line 16\u001b[0m, in \u001b[0;36mpipeline_B\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m conf \u001b[38;5;241m=\u001b[39m (SparkConf()\u001b[38;5;241m.\u001b[39msetAppName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mETL-app-\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(datetime\u001b[38;5;241m.\u001b[39mtoday()))\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.executor.memory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2g\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.mongodb.read.connection.uri\u001b[39m\u001b[38;5;124m\"\u001b[39m,uri)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39msetMaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal[*]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m     )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkIO(conf) \u001b[38;5;28;01mas\u001b[39;00m spark:\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mIngestHadoop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 41\u001b[0m, in \u001b[0;36mIngestHadoop\u001b[0;34m(spark)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m collection \u001b[38;5;129;01min\u001b[39;00m collections:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollection\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m start being Ingested...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m     \u001b[43mbronze_layer_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatabase_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 10\u001b[0m, in \u001b[0;36mbronze_layer_task\u001b[0;34m(spark, db, table_name)\u001b[0m\n\u001b[1;32m      5\u001b[0m hdfs_uri \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhdfs://namenode:8020/bronze_layer/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m uri \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmongodb+srv://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m@python.zynpktu.mongodb.net/?retryWrites=true&w=majority\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m spark_data \u001b[38;5;241m=\u001b[39m (spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmongodb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m           \u001b[38;5;241m.\u001b[39mschema(getSchema(table_name))\n\u001b[0;32m---> 10\u001b[0m           \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muri\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mmongo_uri\u001b[49m)\n\u001b[1;32m     11\u001b[0m           \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatabase\u001b[39m\u001b[38;5;124m'\u001b[39m, database_name)\n\u001b[1;32m     12\u001b[0m           \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcollection\u001b[39m\u001b[38;5;124m'\u001b[39m, table_name)\n\u001b[1;32m     13\u001b[0m           \u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m     14\u001b[0m           \u001b[38;5;241m.\u001b[39mselect([col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m getSchema(table_name)\u001b[38;5;241m.\u001b[39mfieldNames() \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_id\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;66;03m# exclude _id field\u001b[39;00m\n\u001b[1;32m     15\u001b[0m           )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWriting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mongo_uri' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipeline_B()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6b2c652-689b-43a2-b910-4ca0ad8215cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.4.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a4292891-dfb1-4f4a-8d9f-8c11cc64de50;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;10.2.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.8.2 in central\n",
      "\t[4.8.2] org.mongodb#mongodb-driver-sync;[4.8.1,4.8.99)\n",
      "\tfound org.mongodb#bson;4.8.2 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.8.2 in central\n",
      "\tfound org.mongodb#bson-record-codec;4.8.2 in central\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.12/10.2.1/mongo-spark-connector_2.12-10.2.1.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb.spark#mongo-spark-connector_2.12;10.2.1!mongo-spark-connector_2.12.jar (1255ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/4.8.2/mongodb-driver-sync-4.8.2.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#mongodb-driver-sync;4.8.2!mongodb-driver-sync.jar (1174ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/bson/4.8.2/bson-4.8.2.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#bson;4.8.2!bson.jar (4715ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-core/4.8.2/mongodb-driver-core-4.8.2.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#mongodb-driver-core;4.8.2!mongodb-driver-core.jar (17907ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/bson-record-codec/4.8.2/bson-record-codec-4.8.2.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#bson-record-codec;4.8.2!bson-record-codec.jar (252ms)\n",
      ":: resolution report :: resolve 13335ms :: artifacts dl 25312ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.8.2 from central in [default]\n",
      "\torg.mongodb#bson-record-codec;4.8.2 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.8.2 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.8.2 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;10.2.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   5   |   5   |   0   ||   5   |   5   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a4292891-dfb1-4f4a-8d9f-8c11cc64de50\n",
      "\tconfs: [default]\n",
      "\t5 artifacts copied, 0 already retrieved (2370kB/13ms)\n",
      "23/12/02 09:43:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "144543"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user = os.getenv(\"MONGODB_USER\")\n",
    "password = os.getenv(\"MONGODB_PASSWORD\")\n",
    "uri = f\"mongodb+srv://{user}:{password}@python.zynpktu.mongodb.net/?retryWrites=true&w=majority\"\n",
    "\n",
    "conf = (SparkConf().setAppName(\"ETL-app-{}\".format(datetime.today()))\n",
    "        .set(\"spark.executor.memory\", \"2g\")\n",
    "        .set(\"spark.mongodb.read.connection.uri\",uri)\n",
    "        .set(\"spark.mongodb.write.connection.uri\", uri)\n",
    "        .set(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.2.1\")\n",
    "        .setMaster(\"local[*]\")\n",
    "        )\n",
    "\n",
    "table_name = 'tracks_data'\n",
    "hdfs_uri = f\"hdfs://namenode:8020/bronze_layer/{table_name}.parquet\"\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "df = spark.read.parquet(hdfs_uri, inferSchema=True)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d75a5b12-46fd-4e4f-9647-064bc32969d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9806"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2b0e730-b7c7-4d59-b6ae-4d9ac421b24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- danceability: double (nullable = true)\n",
      " |-- energy: double (nullable = true)\n",
      " |-- key: integer (nullable = true)\n",
      " |-- loudness: double (nullable = true)\n",
      " |-- mode: integer (nullable = true)\n",
      " |-- speechiness: double (nullable = true)\n",
      " |-- acousticness: double (nullable = true)\n",
      " |-- instrumentalness: double (nullable = true)\n",
      " |-- liveness: double (nullable = true)\n",
      " |-- valence: double (nullable = true)\n",
      " |-- tempo: double (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- uri: string (nullable = true)\n",
      " |-- track_href: string (nullable = true)\n",
      " |-- analysis_url: string (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      " |-- time_signature: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
