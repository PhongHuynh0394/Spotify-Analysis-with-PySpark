{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37a95990-b807-498e-9b53-7eabfe755261",
   "metadata": {},
   "source": [
    "# PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d11de01-2d68-4e3f-a34b-e757fd0a22ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, BooleanType, StringType, IntegerType, LongType, ArrayType, DateType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb78b08-f2d5-4fc8-abfa-0339adf3adcf",
   "metadata": {},
   "source": [
    "# IO manger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547663a2-cd79-4d1d-9a40-3c722c66e9e6",
   "metadata": {},
   "source": [
    "Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d974d2e-1f57-4624-8a64-8241b91ea991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def SparkIO(conf: SparkConf = SparkConf()):\n",
    "    app_name = conf.get(\"spark.app.name\")\n",
    "    master = conf.get(\"spark.master\")\n",
    "    print(f'Create SparkSession app {app_name} with {master} mode')\n",
    "    spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "    try:\n",
    "        yield spark\n",
    "    finally:\n",
    "        print(f'Stop SparkSession app {app_name}')\n",
    "        spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6828f914-a967-4c98-bc39-cf2a86639d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo.errors import ConnectionFailure\n",
    "from contextlib import contextmanager\n",
    "import os\n",
    "\n",
    "@contextmanager\n",
    "def MongodbIO():\n",
    "    user = os.getenv(\"MONGODB_USER\")\n",
    "    password = os.getenv(\"MONGODB_PASSWORD\")\n",
    "    uri = f\"mongodb+srv://{user}:{password}@python.zynpktu.mongodb.net/?retryWrites=true&w=majority\"\n",
    "    try:\n",
    "        client = MongoClient(uri)\n",
    "        print(f\"MongoDB Connected\")\n",
    "        yield client\n",
    "    except ConnectionFailure:\n",
    "        print(f\"Failed to connect with MongoDB\")\n",
    "        raise ConnectionFailure\n",
    "    finally:\n",
    "        print(\"Close connection to MongoDB\")\n",
    "        client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb9948d-4ad7-46ca-8716-560c613e3e24",
   "metadata": {},
   "source": [
    "## Bronze "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "190ba553-98b8-4884-9899-4b95b7b9e9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSchema(table_name):\n",
    "    \"\"\"This function create Pyspark Schema\"\"\"\n",
    "    artist_schema = StructType([\n",
    "        StructField(\"_id\", StringType(), True),\n",
    "        StructField(\n",
    "            \"external_urls\",\n",
    "            StructType([\n",
    "                StructField(\"spotify\", StringType(), True)\n",
    "            ])\n",
    "        ),\n",
    "        StructField(\n",
    "            \"followers\", \n",
    "            StructType([\n",
    "                StructField(\"href\", StringType(), True),\n",
    "                StructField(\"total\", IntegerType(), True)\n",
    "            ])\n",
    "        ),\n",
    "        StructField(\n",
    "            \"genres\",\n",
    "            ArrayType(StringType(), True)      \n",
    "        ),\n",
    "        StructField(\"href\", StringType(), True),\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\n",
    "            \"images\",\n",
    "            ArrayType(\n",
    "                StructType([\n",
    "                    StructField(\"height\", IntegerType(), True),\n",
    "                    StructField(\"url\", StringType(), True),\n",
    "                    StructField(\"width\", IntegerType(), True)\n",
    "                ])\n",
    "            )\n",
    "        ),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"popularity\", IntegerType(), True),\n",
    "        StructField(\"type\", StringType(), True),\n",
    "        StructField(\"uri\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    album_schema = StructType([\n",
    "        StructField(\"_id\", StringType(), True),\n",
    "        StructField(\"album_type\", StringType(), True),\n",
    "        StructField(\n",
    "            \"copyrights\",\n",
    "            ArrayType(\n",
    "                StructType([\n",
    "                    StructField(\"text\", StringType(), True),\n",
    "                    StructField(\"type\", StringType(), True)\n",
    "                ])\n",
    "            )\n",
    "        ),\n",
    "        StructField(\n",
    "            \"external_ids\",\n",
    "            StructType([\n",
    "                StructField(\"upc\", StringType(), True)\n",
    "            ])\n",
    "        ),\n",
    "        StructField(\n",
    "            \"external_urls\",\n",
    "            StructType([\n",
    "                StructField(\"spotify\", StringType(), True)\n",
    "            ])\n",
    "        ),\n",
    "        StructField(\n",
    "            \"genres\",\n",
    "            ArrayType(StringType(), True)\n",
    "        ),\n",
    "        StructField(\"href\", StringType(), True),\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\n",
    "            \"images\",\n",
    "            ArrayType(\n",
    "                StructType([\n",
    "                    StructField(\"height\", IntegerType(), True),\n",
    "                    StructField(\"url\", StringType(), True),\n",
    "                    StructField(\"width\", IntegerType(), True)\n",
    "                ])\n",
    "            )\n",
    "        ),\n",
    "        StructField(\"label\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"popularity\", IntegerType(), True),\n",
    "        StructField(\"release_date\", StringType(), True),\n",
    "        StructField(\"release_date_precision\", StringType(), True),\n",
    "        StructField(\"total_tracks\", IntegerType(), True),\n",
    "        StructField(\"type\", StringType(), True),\n",
    "        StructField(\"uri\", StringType(), True),\n",
    "        StructField(\"artist_id\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    track_schema = StructType([\n",
    "        StructField(\"_id\", StringType(), True),\n",
    "        StructField(\"disc_number\", IntegerType(), True),\n",
    "        StructField(\"duration_ms\", LongType(), True),\n",
    "        StructField(\"explicit\", BooleanType(), True),\n",
    "        StructField(\n",
    "            \"external_ids\",\n",
    "            StructType([\n",
    "                StructField(\"isrc\", StringType(), True)\n",
    "            ])\n",
    "        ),\n",
    "        StructField(\n",
    "            \"external_urls\",\n",
    "            StructType([\n",
    "                StructField(\"spotify\", StringType(), True)\n",
    "            ])\n",
    "        ),\n",
    "        StructField(\"href\", StringType(), True),\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"is_local\", BooleanType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"popularity\", IntegerType(), True),\n",
    "        StructField(\"preview_url\", StringType(), True),\n",
    "        StructField(\"track_number\", IntegerType(), True),\n",
    "        StructField(\"type\", StringType(), True),\n",
    "        StructField(\"uri\", StringType(), True),\n",
    "        StructField(\"artist_id\", StringType(), True),\n",
    "        StructField(\"album_id\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    track_features_schema = StructType([\n",
    "        StructField(\"_id\", StringType(), True),\n",
    "        StructField(\"danceability\", DoubleType(), True),\n",
    "        StructField(\"energy\", DoubleType(), True),\n",
    "        StructField(\"key\", IntegerType(), True),\n",
    "        StructField(\"loudness\", DoubleType(), True),\n",
    "        StructField(\"mode\", IntegerType(), True),\n",
    "        StructField(\"speechiness\", DoubleType(), True),\n",
    "        StructField(\"acousticness\", DoubleType(), True),\n",
    "        StructField(\"instrumentalness\", DoubleType(), True),\n",
    "        StructField(\"liveness\", DoubleType(), True),\n",
    "        StructField(\"valence\", DoubleType(), True),\n",
    "        StructField(\"tempo\", DoubleType(), True),\n",
    "        StructField(\"type\", StringType(), True),\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"uri\", StringType(), True),\n",
    "        StructField(\"track_href\", StringType(), True),\n",
    "        StructField(\"analysis_url\", StringType(), True),\n",
    "        StructField(\"duration_ms\", LongType(), True),\n",
    "        StructField(\"time_signature\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "    if 'artist' in table_name:\n",
    "        return artist_schema\n",
    "    elif 'album' in table_name:\n",
    "        return album_schema\n",
    "    elif 'feature' in table_name:\n",
    "        return track_features_schema\n",
    "    else:\n",
    "        return track_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75d50f0a-a2a4-4aad-8294-d71a9c7b6b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bronze_layer_task(spark: SparkSession, database_name: str, table_name: str) -> None:\n",
    "    \"\"\"Extract data from MongoDB to HDFS at bronze layer\"\"\"\n",
    "    user = os.getenv(\"MONGODB_USER\")\n",
    "    password = os.getenv(\"MONGODB_PASSWORD\")\n",
    "    hdfs_uri = f\"hdfs://namenode:8020/bronze_layer/{table_name}.parquet\"\n",
    "    mongo_uri = f\"mongodb+srv://{user}:{password}@python.zynpktu.mongodb.net/?retryWrites=true&w=majority\"\n",
    "    \n",
    "    spark_data = (spark.read.format(\"mongodb\")\n",
    "              .schema(getSchema(table_name))\n",
    "              .option(\"uri\", mongo_uri)\n",
    "              .option('database', database_name)\n",
    "              .option('collection', table_name)\n",
    "              .load()\n",
    "              .select([col for col in getSchema(table_name).fieldNames() if col != '_id'])\n",
    "              )\n",
    "    # Exclude _id field\n",
    "    print(f\"Writing {table_name}\")\n",
    "    try:\n",
    "        spark_data.write.parquet(hdfs_uri, mode=\"overwrite\")\n",
    "        print(f\"Bronze: Successfully push {table_name}.parquet\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(spark_data.printSchema())\n",
    "        # spark_data = (spark.read.format(\"mongodb\")\n",
    "        #       .schema(getSchema(table_name))\n",
    "        #       .option(\"uri\", mongo_uri)\n",
    "        #       .option('database', database_name)\n",
    "        #       .option('collection', table_name)\n",
    "        #       .load()\n",
    "        #       .select([col for col in getSchema(table_name).fieldNames() if col != '_id'])\n",
    "        #       )\n",
    "        # spark_data.write.parquet(hdfs_uri, mode=\"overwrite\")\n",
    "        # print(f\"Bronze: Successfully push {table_name}.parquet\")\n",
    "\n",
    "def IngestHadoop(spark: SparkSession):\n",
    "    \"\"\"Extract data From MongoDb and Load to HDFS\"\"\"\n",
    "\n",
    "    # Connect to MongoDB\n",
    "\n",
    "    # database_name = \"remake_spotify_crawling_data\"\n",
    "    database_name = os.getenv(\"MONGODB_DATABASE\")\n",
    "    \n",
    "    \n",
    "    with MongodbIO() as client:\n",
    "        mongo_db = client[database_name] \n",
    "        collections = mongo_db.list_collection_names() #get all collectons\n",
    "    \n",
    "        #Running task concurrently\n",
    "        for collection in collections:\n",
    "            print(f\"{collection} start being Ingested...\")\n",
    "            bronze_layer_task(spark, database_name, collection) #collection is also the name of table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a68bbd-bcdc-436a-8233-c1631e7dcc5d",
   "metadata": {},
   "source": [
    "There is 4 tables:\n",
    "- artists_data.parquet\n",
    "- songs_data.parquet\n",
    "- genres_data.parquet\n",
    "- albums_data.parquet\n",
    "\n",
    "location: hdfs://namenode:8020/bronze_layer/{table_name}.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae751864-38fa-439c-86ba-25f8c5d0c5ef",
   "metadata": {},
   "source": [
    "## Silver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf39cf0-e570-41a0-be62-98be23c834a7",
   "metadata": {},
   "source": [
    "### Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e29de1-4be3-4ea6-a908-ab545adb3c48",
   "metadata": {},
   "source": [
    "![Schema](./spotify.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c92ca9-4f92-461c-819a-2f6c2c70278a",
   "metadata": {},
   "source": [
    "Target: \n",
    "- using pyspark Cleaning, droping duplicated, drop unusable column (Read [EDA](https://colab.research.google.com/drive/15uM8Uvj1I89zjtJrVn-Z7mvkfyCWo50T?usp=sharing)), format type, there are many duplicated observation.\n",
    "- join dim artist and dim albums -> join_artist_albums table (clean table before merge) (task 1)\n",
    "- clean genre, then write back to silver(task 2) -> clean_genre table\n",
    "- clean songs (task 3) -> clean_songs table (return None)\n",
    "- The location of silver: hdfs_uri = f\"hdfs://namenode:8020/silver_layer/{table_name}.parquet\" with table_name is name of result table\n",
    "\n",
    "\n",
    "Requirements:\n",
    "- Input of silver main task (spark session), Output: None\n",
    "- silver main task may have many child tasks, concurrently or sequencially\n",
    "- Child task input (spark session), any extended params or return base on you, ensure write back result in hdfs with related uri\n",
    "- Writing (print out) logs every action, handle error and exception (raise it if neccesary)\n",
    "\n",
    "Dont forget to add your main task to main function !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d88006a-e33c-403e-b407-1135f16817de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run some code here\n",
    "def silver_layer_task(spark: SparkSession):\n",
    "    '''Do some Cleaning tasks for silver layer'''\n",
    "    # task 1\n",
    "    # task 2\n",
    "    # task 3 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dabde27-5933-4ef3-8175-e04b07d83cfd",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fb1e0e4-6556-4a1c-a220-acd50f895285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_B():\n",
    "    \"\"\"ELT pipeline with pyspark\"\"\"\n",
    "\n",
    "    user = os.getenv(\"MONGODB_USER\")\n",
    "    password = os.getenv(\"MONGODB_PASSWORD\")\n",
    "    uri = f\"mongodb+srv://{user}:{password}@python.zynpktu.mongodb.net/?retryWrites=true&w=majority\"\n",
    "    conf = (SparkConf().setAppName(\"ETL-app-{}\".format(datetime.today()))\n",
    "        .set(\"spark.executor.memory\", \"2g\")\n",
    "        .set(\"spark.mongodb.read.connection.uri\",uri)\n",
    "        .set(\"spark.mongodb.write.connection.uri\", uri)\n",
    "        .set(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.2.1\")\n",
    "        .setMaster(\"local[*]\")\n",
    "        )\n",
    "\n",
    "    with SparkIO(conf) as spark:\n",
    "        IngestHadoop(spark) # <----- bronze task\n",
    "        # add silver tasks here <-------\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f14c330-c89d-4a78-99b7-dc627e2b1704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create SparkSession app ETL-app-2023-12-04 08:27:30.473763 with local[*] mode\n",
      "MongoDB Connected\n",
      "tracks_features_data start being Ingested...\n",
      "Writing tracks_features_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze: Successfully push tracks_features_data.parquet\n",
      "artists_data start being Ingested...\n",
      "Writing artists_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze: Successfully push artists_data.parquet\n",
      "tracks_data start being Ingested...\n",
      "Writing tracks_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze: Successfully push tracks_data.parquet\n",
      "albums_data start being Ingested...\n",
      "Writing albums_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze: Successfully push albums_data.parquet\n",
      "Close connection to MongoDB\n",
      "Stop SparkSession app ETL-app-2023-12-04 08:27:30.473763\n",
      "CPU times: user 111 ms, sys: 126 ms, total: 236 ms\n",
      "Wall time: 40.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipeline_B()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6b2c652-689b-43a2-b910-4ca0ad8215cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user = os.getenv(\"MONGODB_USER\")\n",
    "password = os.getenv(\"MONGODB_PASSWORD\")\n",
    "uri = f\"mongodb+srv://{user}:{password}@python.zynpktu.mongodb.net/?retryWrites=true&w=majority\"\n",
    "\n",
    "conf = (SparkConf().setAppName(\"ETL-app-{}\".format(datetime.today()))\n",
    "        .set(\"spark.executor.memory\", \"2g\")\n",
    "        .set(\"spark.mongodb.read.connection.uri\",uri)\n",
    "        .set(\"spark.mongodb.write.connection.uri\", uri)\n",
    "        .set(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.2.1\")\n",
    "        .setMaster(\"local[*]\")\n",
    "        )\n",
    "\n",
    "table_name = 'artists_data'\n",
    "hdfs_uri = f\"hdfs://namenode:8020/bronze_layer/{table_name}.parquet\"\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "df = spark.read.parquet(hdfs_uri, inferSchema=True)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d75a5b12-46fd-4e4f-9647-064bc32969d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9806"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2b0e730-b7c7-4d59-b6ae-4d9ac421b24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- danceability: double (nullable = true)\n",
      " |-- energy: double (nullable = true)\n",
      " |-- key: integer (nullable = true)\n",
      " |-- loudness: double (nullable = true)\n",
      " |-- mode: integer (nullable = true)\n",
      " |-- speechiness: double (nullable = true)\n",
      " |-- acousticness: double (nullable = true)\n",
      " |-- instrumentalness: double (nullable = true)\n",
      " |-- liveness: double (nullable = true)\n",
      " |-- valence: double (nullable = true)\n",
      " |-- tempo: double (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- uri: string (nullable = true)\n",
      " |-- track_href: string (nullable = true)\n",
      " |-- analysis_url: string (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      " |-- time_signature: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
